Actor MLP: Sequential(
  (0): Linear(in_features=3617, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=3, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=3617, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
Backend TkAgg is interactive backend. Turning interactive mode on.
/home/kappi/.local/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
Epoch [1/10], Batch [1/125], Loss: 0.2228235900402069
Epoch [1/10], Batch [11/125], Loss: 0.26746416091918945
Epoch [1/10], Batch [21/125], Loss: 0.2521076202392578
Epoch [1/10], Batch [31/125], Loss: 0.1857217401266098
Epoch [1/10], Batch [41/125], Loss: 0.12712255120277405
Epoch [1/10], Batch [51/125], Loss: 0.3138854205608368
Epoch [1/10], Batch [61/125], Loss: 0.11595126986503601
Epoch [1/10], Batch [71/125], Loss: 0.13748537003993988
Epoch [1/10], Batch [81/125], Loss: 0.20718488097190857
Epoch [1/10], Batch [91/125], Loss: 0.1516227424144745
Epoch [1/10], Batch [101/125], Loss: 0.1893129050731659
Epoch [1/10], Batch [111/125], Loss: 0.17248651385307312
Epoch [1/10], Batch [121/125], Loss: 0.0992073267698288
Epoch [2/10], Batch [1/125], Loss: 0.10569378733634949
Epoch [2/10], Batch [11/125], Loss: 0.08959456533193588
Epoch [2/10], Batch [21/125], Loss: 0.05547469109296799
Epoch [2/10], Batch [31/125], Loss: 0.12873172760009766
Epoch [2/10], Batch [41/125], Loss: 0.2089175581932068
Epoch [2/10], Batch [51/125], Loss: 0.22356000542640686
Epoch [2/10], Batch [61/125], Loss: 0.20494213700294495
Epoch [2/10], Batch [71/125], Loss: 0.21178577840328217
Epoch [2/10], Batch [81/125], Loss: 0.1056889146566391
Epoch [2/10], Batch [91/125], Loss: 0.10736633837223053
Epoch [2/10], Batch [101/125], Loss: 0.07562783360481262
Epoch [2/10], Batch [111/125], Loss: 0.21579186618328094
Epoch [2/10], Batch [121/125], Loss: 0.10163241624832153
Epoch [3/10], Batch [1/125], Loss: 0.16160909831523895
Epoch [3/10], Batch [11/125], Loss: 0.06630794703960419
Epoch [3/10], Batch [21/125], Loss: 0.22160348296165466
Epoch [3/10], Batch [31/125], Loss: 0.10115443170070648
Epoch [3/10], Batch [41/125], Loss: 0.0891806110739708
Epoch [3/10], Batch [51/125], Loss: 0.12483259290456772
Epoch [3/10], Batch [61/125], Loss: 0.11266805231571198
Epoch [3/10], Batch [71/125], Loss: 0.16491566598415375
Epoch [3/10], Batch [81/125], Loss: 0.14238455891609192
Epoch [3/10], Batch [91/125], Loss: 0.11857514083385468
Epoch [3/10], Batch [101/125], Loss: 0.10660098493099213
Epoch [3/10], Batch [111/125], Loss: 0.0891614779829979
Epoch [3/10], Batch [121/125], Loss: 0.118886798620224
Epoch [4/10], Batch [1/125], Loss: 0.11711355298757553
Epoch [4/10], Batch [11/125], Loss: 0.27896758913993835
Epoch [4/10], Batch [21/125], Loss: 0.14616194367408752
Epoch [4/10], Batch [31/125], Loss: 0.07159128785133362
Epoch [4/10], Batch [41/125], Loss: 0.07579846680164337
Epoch [4/10], Batch [51/125], Loss: 0.09976788610219955
Epoch [4/10], Batch [61/125], Loss: 0.04964342713356018
Epoch [4/10], Batch [71/125], Loss: 0.1633942425251007
Epoch [4/10], Batch [81/125], Loss: 0.12683388590812683
Epoch [4/10], Batch [91/125], Loss: 0.12757128477096558
Epoch [4/10], Batch [101/125], Loss: 0.06549088656902313
Epoch [4/10], Batch [111/125], Loss: 0.1649298071861267
Epoch [4/10], Batch [121/125], Loss: 0.09880159795284271
Epoch [5/10], Batch [1/125], Loss: 0.06489860266447067
Epoch [5/10], Batch [11/125], Loss: 0.07221443951129913
Epoch [5/10], Batch [21/125], Loss: 0.04483118653297424
Epoch [5/10], Batch [31/125], Loss: 0.07709155231714249
Epoch [5/10], Batch [41/125], Loss: 0.05703537166118622
Epoch [5/10], Batch [51/125], Loss: 0.10010264813899994
Epoch [5/10], Batch [61/125], Loss: 0.1425449103116989
Epoch [5/10], Batch [71/125], Loss: 0.02888401970267296
Epoch [5/10], Batch [81/125], Loss: 0.017685046419501305
Epoch [5/10], Batch [91/125], Loss: 0.05008215829730034
Epoch [5/10], Batch [101/125], Loss: 0.1295125037431717
Epoch [5/10], Batch [111/125], Loss: 0.0635206550359726
Epoch [5/10], Batch [121/125], Loss: 0.26159268617630005
Epoch [6/10], Batch [1/125], Loss: 0.07887080311775208
Epoch [6/10], Batch [11/125], Loss: 0.02525988593697548
Epoch [6/10], Batch [21/125], Loss: 0.0545138381421566
Epoch [6/10], Batch [31/125], Loss: 0.035394664853811264
Epoch [6/10], Batch [41/125], Loss: 0.02717611752450466
Epoch [6/10], Batch [51/125], Loss: 0.013368646614253521
Epoch [6/10], Batch [61/125], Loss: 0.01951281726360321
Epoch [6/10], Batch [71/125], Loss: 0.03285183012485504
Epoch [6/10], Batch [81/125], Loss: 0.031204640865325928
Epoch [6/10], Batch [91/125], Loss: 0.033159445971250534
Epoch [6/10], Batch [101/125], Loss: 0.03355546295642853
Epoch [6/10], Batch [111/125], Loss: 0.06768567115068436
Epoch [6/10], Batch [121/125], Loss: 0.12289702892303467
Epoch [7/10], Batch [1/125], Loss: 0.043433189392089844
Epoch [7/10], Batch [11/125], Loss: 0.07253748923540115
Epoch [7/10], Batch [21/125], Loss: 0.04734154790639877
Epoch [7/10], Batch [31/125], Loss: 0.02958592399954796
Epoch [7/10], Batch [41/125], Loss: 0.021790098398923874
Epoch [7/10], Batch [51/125], Loss: 0.022687815129756927
Epoch [7/10], Batch [61/125], Loss: 0.03390376269817352
Epoch [7/10], Batch [71/125], Loss: 0.024870816618204117
Epoch [7/10], Batch [81/125], Loss: 0.052132852375507355
Epoch [7/10], Batch [91/125], Loss: 0.03280695527791977
Epoch [7/10], Batch [101/125], Loss: 0.0987565815448761
Epoch [7/10], Batch [111/125], Loss: 0.0212581567466259
Epoch [7/10], Batch [121/125], Loss: 0.025906255468726158
Epoch [8/10], Batch [1/125], Loss: 0.013554327189922333
Epoch [8/10], Batch [11/125], Loss: 0.022306477651000023
Epoch [8/10], Batch [21/125], Loss: 0.01004786603152752
Epoch [8/10], Batch [31/125], Loss: 0.050812363624572754
Epoch [8/10], Batch [41/125], Loss: 0.034008026123046875
Epoch [8/10], Batch [51/125], Loss: 0.011125238612294197
Epoch [8/10], Batch [61/125], Loss: 0.02023005299270153
Epoch [8/10], Batch [71/125], Loss: 0.037931229919195175
Epoch [8/10], Batch [81/125], Loss: 0.023531999439001083
Epoch [8/10], Batch [91/125], Loss: 0.06590832769870758
Epoch [8/10], Batch [101/125], Loss: 0.03726623207330704
Epoch [8/10], Batch [111/125], Loss: 0.017091069370508194
Epoch [8/10], Batch [121/125], Loss: 0.022239550948143005
Epoch [9/10], Batch [1/125], Loss: 0.0331852026283741
Epoch [9/10], Batch [11/125], Loss: 0.011478736996650696
Epoch [9/10], Batch [21/125], Loss: 0.014843404293060303
Epoch [9/10], Batch [31/125], Loss: 0.009477509185671806
Epoch [9/10], Batch [41/125], Loss: 0.023325320333242416
Epoch [9/10], Batch [51/125], Loss: 0.014919131994247437
Epoch [9/10], Batch [61/125], Loss: 0.008913065306842327
Epoch [9/10], Batch [71/125], Loss: 0.013874979689717293
Epoch [9/10], Batch [81/125], Loss: 0.02166808769106865
Epoch [9/10], Batch [91/125], Loss: 0.035818565636873245
Epoch [9/10], Batch [101/125], Loss: 0.006661903113126755
Epoch [9/10], Batch [111/125], Loss: 0.02287231758236885
Epoch [9/10], Batch [121/125], Loss: 0.012117788195610046
Epoch [10/10], Batch [1/125], Loss: 0.008683972992002964
Epoch [10/10], Batch [11/125], Loss: 0.009034795686602592
Epoch [10/10], Batch [21/125], Loss: 0.014277607202529907
Epoch [10/10], Batch [31/125], Loss: 0.018528243526816368
Epoch [10/10], Batch [41/125], Loss: 0.012379836291074753
Epoch [10/10], Batch [51/125], Loss: 0.015902873128652573
Epoch [10/10], Batch [61/125], Loss: 0.012425589375197887
Epoch [10/10], Batch [71/125], Loss: 0.005997064523398876
Epoch [10/10], Batch [81/125], Loss: 0.0054776715114712715
Epoch [10/10], Batch [91/125], Loss: 0.009879585355520248
Epoch [10/10], Batch [101/125], Loss: 0.006526430603116751
Epoch [10/10], Batch [111/125], Loss: 0.007265610154718161
Epoch [10/10], Batch [121/125], Loss: 0.007478469051420689
Training completed.